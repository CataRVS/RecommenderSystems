import os
import pandas as pd
from tqdm import tqdm
from typing import Dict, Tuple, Any, List

from src.datamodule.data import Data
from src.utils.utils import set_seed
import src.evaluation.evaluation as ev


"""
This script evaluates recommendation files generated by different recommender systems.
Complete the required parameters in the main section and run the script to execute the evaluation.
"""

# Supported metrics
METRICS = ["precision", "recall", "ndcg", "epc", "gini", "aggregate_diversity"]
RECS_SEP = ","
DEFAULT_COLS = ["user", "item", "rating", "timestamp"]


def load_data(
    train_path: str,
    test_path: str,
    sep: str,
    test_size: float,
    ignore_first: bool,
    seed: int = 42,
) -> Data:
    """
    Load the training and testing data from the specified paths.

    Parameters:
        train_path (str): Path to the training data file.
        test_path (str): Path to the testing data file or "none" if not used.
        sep (str): Separator used in the data files.
        test_size (float): Proportion of the data to include in the test split.
        ignore_first (bool): Whether to ignore the first line of the data files.
        seed (int): Random seed for reproducibility.

    Returns:
        Data: An instance of the Data class containing the loaded data.
    """
    # Set the random seed so the test split is reproducible
    set_seed(seed)
    try:
        data = Data(
            data_path_train=train_path,
            data_path_test=test_path if test_path.lower() != "none" else "none",
            sep=sep,
            test_size=test_size,
            ignore_first_line=ignore_first,
            col_names=DEFAULT_COLS,
        )
        return data
    except FileNotFoundError as e:
        print(e)
        exit(1)
    except pd.errors.ParserError as e:
        print(e)
        exit(1)


def load_evaluation_metric(metric_name: str, data: Data) -> ev.Evaluation:
    """
    Load the evaluation metric based on the provided metric name.
    Available metrics are "precision", "recall", "ndcg", "epc", "gini", and "aggregate_diversity".

    Parameters:
        metric_name (str): Name of the metric to load.
        data (Data): An instance of the Data class containing the data.

    Returns:
        Evaluation: An instance of the evaluation metric class corresponding to the metric name.
    """
    match metric_name:
        case "precision": return ev.Precision(data)
        case "recall": return ev.Recall(data)
        case "ndcg": return ev.NDCG(data)
        case "epc": return ev.EPC(data)
        case "gini": return ev.Gini(data)
        case "aggregate_diversity": return ev.AggregateDiversity(data)
        case _:
            print(f"Evaluation metric '{metric_name}' not found. Check the available metrics.")
            exit(1)


def parse_filename(filename: str) -> Tuple[str, str, Dict[str, Any], str]:
    """
    Parse the filename to extract recommender, strategy, parameters, and configuration.

    Parameters:
        filename (str): The name of the recommendation file.

    Returns:
        tuple: A tuple containing the recommender name, strategy, parameters as a dictionary,
             and a configuration string.
    """
    # Obtain the base name of the file and split it into parts
    base = os.path.basename(filename).replace(".csv", "")
    parts = base.split("_")

    # For each recommender type, extract the relevant information
    # The MF and BPRMF recommender types have the same structure
    if base.startswith("mf") or base.startswith("bprmf"):
        recommender = parts[0]
        strategy = parts[2] + "_" + parts[3]
        n_factors = int(parts[4][1:])
        lr = float(parts[5][2:].replace("p", "."))
        reg = float(parts[6][3:].replace("p", "."))
        epochs = int(parts[7][2:])
        batch_size = int(parts[8][2:])
        params = {
            "n_factors": n_factors,
            "learning_rate": lr,
            "regularization": reg,
            "epochs": epochs,
            "batch_size": batch_size,
        }
        config = f"f{n_factors}_lr{lr}_reg{reg}_ep{epochs}_bs{batch_size}"

    # The knn_user and knn_item recommender types have the same structure
    elif base.startswith("knn_user") or base.startswith("knn_item"):
        recommender = parts[0] + "_" + parts[1]
        strategy = parts[3] + "_" + parts[4]
        k = int(parts[5][1:])
        similarity = parts[6]
        params = {"k": k, "similarity": similarity}
        config = f"k{k}_{similarity}"

    elif base.startswith("mlp"):
        recommender = parts[0]
        strategy = parts[2] + "_" + parts[3]
        n_factors = int(parts[4][1:])
        lr = float(parts[5][2:].replace("p", "."))
        hidden_dims = tuple(int(x) for x in parts[6][2:].split("-"))
        reg = float(parts[7][3:].replace("p", "."))
        epochs = int(parts[8][2:])
        batch_size = int(parts[9][2:])
        dropout = float(parts[10][4:].replace("p", "."))
        params = {
            "n_factors": n_factors,
            "learning_rate": lr,
            "hidden_dims": hidden_dims,
            "regularization": reg,
            "epochs": epochs,
            "batch_size": batch_size,
            "dropout": dropout,
        }
        config = f"f{n_factors}_lr{lr}_hd{'-'.join(map(str, hidden_dims))}_reg{reg}_ep{epochs}_bs{batch_size}"

    else:
        recommender = parts[0]
        strategy = parts[2] + "_" + parts[3]
        params = {}
        config = ""

    return recommender, strategy, params, config


def evaluate_file(file_path: str, data: Data) -> Dict[str, float]:
    """
    Evaluate a recommendation file using the specified metrics.

    Parameters:
        file_path (str): Path to the recommendation file.
        data (Data): An instance of the Data class containing the data.

    Returns:
        dict: A dictionary containing the evaluation scores for each metric.
    """
    scores = {}
    # For each metric:
    for metric in METRICS:
        # Load the evaluation metric
        evaluator = load_evaluation_metric(metric, data)
        # Evaluate the recommendations in the file
        score = evaluator.evaluate(
            recommendations_path=file_path,
            recommendations_sep=RECS_SEP,
            ignore_first_line=False,
        )
        # Store the score in the scores dictionary
        scores[metric] = score
    return scores


def export_results_to_csv(
    results: List[Tuple[str, Tuple[str, str, Dict[str, Any], str, Dict[str, float]]]],
    save_path: str,
    model: str
) -> str:
    """
    Export the evaluation results to a CSV file.

    Parameters:
        results (list): List of tuples containing file name, info and evaluation scores.
        save_path (str): Path to save the CSV file.
        model (str): The name of the model being evaluated.

    Returns:
        str: Path to the saved CSV file.
    """
    rows = []
    # For each file and its evaluation results:
    for file, (recommender, strategy, params, config, scores) in results:
        # Create a row with the file name, recommender, strategy, parameters, and scores
        row = {
            "filename": file,
            "recommender": recommender,
            "strategy": strategy,
            "config": config,
        }
        # Add the parameters and scores to the row
        row.update(params)
        row.update(scores)
        # Add the row to the list of rows
        rows.append(row)

    # Create the output directory if it does not exist
    path = os.path.join(save_path, f"{model}_evaluation_results.csv")
    # Create a DataFrame from the rows and save it to a CSV file
    df = pd.DataFrame(rows)
    # Save the DataFrame to a CSV file
    df.to_csv(path, index=False)
    # Return the path to the saved CSV file
    return path


def run_evaluation(
    model: str,
    recs_dir: str,
    train_path: str,
    test_path: str,
    test_size: float,
    output_dir: str,
    sep: str = "\t",
    seed: int = 42
):
    """
    Run the evaluation of recommendation files and save the results in a CSV file.

    Parameters:
        model (str): The name of the model to evaluate ("popularity", "random", "mf", "bprmf",
            "knn_user", "knn_item").
        recs_dir (str): Directory containing recommendation files.
        train_path (str): Path to the training data file.
        test_path (str): Path to the testing data file or "none" if not used.
        test_size (float): Proportion of the data to include in the test split.
        output_dir (str): Directory to save the evaluation results.
        sep (str): Separator used in the recommendation files.
        seed (int): Random seed for reproducibility.
    """
    # Verify that the model is valid
    if model not in {"popularity", "random", "mf", "bprmf", "knn_user", "knn_item", "mlp"}:
        print("Model must be one of 'popularity', 'random', 'mf', 'bprmf', 'knn_user', 'knn_item'.")
        exit(1)

    print(f"\nEvaluating recommendations for model: {model.upper()}")

    # Load the data
    try:
        data = load_data(train_path, test_path, sep, test_size, False, seed)
    except Exception as e:
        print(e)
        exit(1)

    results = []

    # Check if the recommendations directory exists and contains files
    try:
        # List all files in the recommendations directory
        recs = sorted(os.listdir(recs_dir))
        # Select only files that match the model and are CSV files
        recs = [f for f in recs if f.endswith(".csv") and f.startswith(model)]
        if not recs:
            print(f"No recommendation files found in: {recs_dir}")
            exit(1)
    except FileNotFoundError:
        print("Please check the recommendations directory path.")
        exit(1)

    # For the found recommendation files:
    for file in tqdm(recs, desc="Evaluating files", unit="file"):
        # Construct the full path to the recommendation file
        full_path = os.path.join(recs_dir, file)
        try:
            # Obtain the recommender, strategy, parameters, and configuration from the filename
            recommender, strategy, params, config = parse_filename(file)
        except ValueError and IndexError:
            print(f"Error parsing filename '{file}'")
            continue
        # Evaluate the recommendation file
        scores = evaluate_file(full_path, data)
        # Append the results to the results list
        results.append((file, (recommender, strategy, params, config, scores)))

    # If no results were collected, exit
    if not results:
        print("No valid recommendation files found or evaluated.")
        exit(1)

    # Obtain the final folder of the recommendations directory
    dataset_folder = os.path.relpath(recs_dir, "recommendations").split(os.sep)[-1]
    # Verify that the output directory exists ends with "/"
    if not output_dir.endswith("/"):
        output_dir += "/"
    # Add the dataset folder to the output directory
    output_path = output_dir + dataset_folder + "/"
    # Make sure the output directory exists
    os.makedirs(output_path, exist_ok=True)

    # Save the results to a CSV file in the output directory
    results_file = export_results_to_csv(results, output_path, model)
    print(f"\nEvaluation results saved to: {results_file}")

    # Preview the 5 best results
    df = pd.read_csv(results_file)
    df = df.sort_values(by='precision', ascending=False).reset_index(drop=True)
    best_results = df[
        ['filename', 'precision', 'recall', 'ndcg', 'epc', 'gini', 'aggregate_diversity']
    ].head(5)
    print("\nTop 5 Results:")
    print(best_results)


if __name__ == "__main__":
    ########## CONFIGURATION ##########
    model = "mlp"  # Options "popularity", "random", "knn_user", "knn_item", "mf", "bprmf", "mlp"
    recs_dir = "results/recommendations/dataset"
    train_path = "data/dataset/train.txt"  # Path to the training data file
    test_path = "data/dataset/test.txt"  # "none" or provide a path to a test file

    # For my use:
    train_path = "data/ml-100k/u1.base"  # Path to the training data file
    test_path = "data/ml-100k/u1.test"  # "none" or provide a path to a test 
    # train_path = "data/NewYork/US_NewYork_Processed_Shortened_10.txt"  # Path to the training data file
    # test_path = "none"  # "none" or provide a path to a test file
    test_size = 0.1
    sep = "\t"
    output_dir = "results/metrics/"

    run_evaluation(
        model=model,
        recs_dir=recs_dir,
        train_path=train_path,
        test_path=test_path,
        test_size=test_size,
        output_dir=output_dir,
        sep=sep
    )
